---
layout: post
title: PINN vs FEM II - Theoretical connection between FEM and Single-Layer Perceptron
date: 2026-02-26 12:00:00
description: The shared origin of PINN and FEM
tags: PINN
categories: PINN vs FEM
thumbnail: assets/img/NNvsFEM.png
authors:
  - name: Sean De Marco
    affiliations:
      name: Aeronautics, Imperial
  - name: Lloyd Fung
---

The mathematics behind neural networks is often seen as a black-box, with PINNs being similar. That was definitely my impression when I first interacted with PINNs. But we can break down a Neural Network (NN) all the way to its most basic consistuent unit, a Single-Layer Perceptron (SLP), then we can start to uncover how it acutally works. And I promise, after reading this, you'll find a whole new appreciation of its similarity with older more established.

(As a side note, the universal approximation theorem was proven also on the basis of a SLP. If you're familiar with that theorem, you'll find that it is not too disimilar to how FEM approximate PDE solutions too.)

## Single-Layer Perceptron 

![A Single-Layer Perceptron](assets/imgs/NNvsFEM/SLP.png)

The Single-Layer Perceptron is the most simple NN one can write down. In the simple case of approximation a function $$u(x)$$, for example, the $$x$$ will be your input, and $$\tilde{u}$$ the approximated output. This SLP can be written out explicitly as

$$
\tilde{u}(x) = \bm{W}^{(2)} \, \sigma^{(1)} \left( \bm{W}^{(1)} {x} + \bm{b}^{(1)} \right) + b^{(2)} 
$$

where, for $H$ hidden nodes, 

$$
\bm{W}^{(1)} \in \mathbb{R}^{H \times 1}, \quad 
x \in \mathbb{R}^{1 \times N}, \quad 
\bm{b}^{(1)} \in \mathbb{R}^{H \times 1}, \quad 
\bm{W}^{(2)} \in \mathbb{R}^{1 \times H}, \quad 
b^{(2)} \in \mathbb{R}^{1 \times 1}. \tag{16}
$$

The activation $$\sigma$$ here can be anything from $$\tanh$$ to ReLU, applied elementwise. The universal approximation theorem tells us that, as long as $$\sigma$$ fulfill the "discriminatory" condition, with enough $H$, this unit can approximate any function of arbitrary complexity. 

Normally, to train a SLP like this, we treat all the weights $$\bm{W}^{(*)}$$ and biases $$\bm{b}^{(*)}$$ as trainable parameters to the optimisation that minimise the loss, in this case, the difference between the SLP evaluation and the data at sampled points $$x_i$$, i.e. $$\Sum_i || u(x_i) - \tilde{u}(x_i) || $$. 

However, to make the comparison more appearant, we are going to make one additional trick: we are going to fix all parameters except the last layer weight $$\bm{W}^{(2)}$$ (or $$n$$ if you have $$n$$ layers), and put the last layer bias $$b^{(2)}$$ to zero. Putting $$b^{(2)}$$ is a fairly straight forward simplification, since it simply shift the function by a fixed value. However, fixing everythign except $$\bm{W}^{(2)}$$ might come as a surprise - why would you want to do this?

Well, as it turns out, if only the last layer is trained, there is a much faster way to train if the loss is simply an L2-Norm - We can use linear regression to get $$\bm{W}^{(2)}$$ ! This technique is known as Extreme Learning Machine (ELM). We don't have time to do a deep dive into ELMs. All you need to know is that this trick enables regression-styled training.

## Finite Element Methods
In Finite Elements, we typically approximate $$u(x)$$ by a linear sum of the weighted basis functions belonging to each element such that

$$
\hat{u}(x) = \sum_{j=1}^{N}U_j \varphi_j \tag{2}
$$

where $$\hat{u}(x)$$ is the approximation to $$u(x)$$ at a given $$x$$. The  $$U_j$$ and $$\varphi_j$$ are the weights and trial basis functions respectively. Now, it is possible to select any trial basis function. Here we choose the lowest order one - piecewise-linear basis function:

$$
\varphi_j(x) =
\begin{cases}
\frac{x - x_{j-1}}{x_j - x_{j-1}}, & x_{j-1} < x < x_j \\
\frac{x_{j+1} - x}{x_{j+1} - x_j}, & x_j < x < x_{j+1}
\end{cases} \tag{6}
$$

If we further assume that the FEM is done on a equispaced grid, i.e. $$x_j - x_{j-1} = \Delta x$$ for all $j$, then we can simplify the basis function as 

$$
\varphi_j(x) =
\begin{cases}
\frac{x - b_{j} + \Delta x }{\Delta x}, & b_j - \Delta x< x < b_j \\
\frac{b_{j} + \Delta x - x}{\Delta x}, & b_j < x < b_j + \Delta x
\end{cases} \tag{6}
$$

or

$$
\varphi_j(x) =\varphi(x - b_j); \; 
\varphi(x) =
\begin{cases}
\frac{x + \Delta x}{\Delta x}, & - \Delta x< x < 0\\
\frac{\Delta x - x}{\Delta x}, & 0 < x < Delta x
\end{cases}
$$

## The Comparison
Notice the similiarity yet? 

## What about the test functions? What's that equivalent to?